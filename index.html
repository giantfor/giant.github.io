<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" href="/static/img/favicon.ico" /><title>Machine Learning : Gradient Descent - Stack Problems</title><meta name="author" content="Agung Setiaji" /><meta name="description" content="Machine Learning : Gradient Descent" /><meta name="keywords" content="Machine Learning : Gradient Descent, Stack Problems, python, machine learning" /><link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml"><meta content="1749788565247320" property="fb:app_id"><meta content="Stack Problems" property="og:site_name"><meta content="Machine Learning : Gradient Descent" property="og:title"><meta content="article" property="og:type"><meta content="My Personal Stack Problems" property="og:description"><meta content="https://mragungsetiaji.github.io/python/machine%20learning/2018/09/16/gradient-descent.html" property="og:url"><meta content="2018-09-16T11:34:10+00:00" property="article:published_time"><meta content="https://mragungsetiaji.github.io/about/" property="article:author"><meta content="https://mragungsetiaji.github.io/static/img/avatar.jpg" property="og:image"><meta content="python" property="article:section"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@github.io"><meta name="twitter:creator" content="@github.io"><meta name="twitter:title" content="Machine Learning : Gradient Descent"><meta name="twitter:url" content="https://mragungsetiaji.github.io/python/machine%20learning/2018/09/16/gradient-descent.html"><meta name="twitter:description" content="My Personal Stack Problems"> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous"><link rel="stylesheet" href="/static/css/syntax.css"><link href="/static/css/bootstrap.min.css" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,300italic,300,400italic,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css"><link rel="stylesheet" href="/static/css/super-search.css"><link rel="stylesheet" href="/static/css/thickbox.css"><link rel="stylesheet" href="/static/css/projects.css"><link rel="stylesheet" href="/static/css/main.css"> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-115104999-1', 'auto'); ga('send', 'pageview'); </script><body><div class="container"><div class="col-sm-3"><div class="fixed-condition"> <a href="/"><img class="profile-avatar" src="/static/img/avatar.jpg" height="75px" width="75px" /></a><h1 class="author-name">Agung Setiaji</h1><div class="profile-about"> Data Science and Analytics</div><div class="social"><ul><li><a href="https://facebook.com/mragungsetiaji" target="_blank"><i class="fa fa-facebook"></i></a><li><a href="https://twitter.com/mragungsetiaji" target="_blank"><i class="fa fa-twitter"></i></a><li><a href="https://linkedin.com/in/mragungsetiaji" target="_blank"><i class="fa fa-linkedin"></i></a><li><a href="#" target="_blank"><i class="fa fa-stack-exchange"></i></a><li><a href="https://github.com/mragungsetiaji" target="_blank"><i class="fa fa-github"></i></a></ul></div><div class="search" id="js-search"> <input type="text" placeholder="(sitemap)~$ type to search" class="search__input form-control" id="js-search__input"><ul class="search__results" id="js-search__results"></ul></div><hr /><ul class="sidebar-nav"> <strong>Navigation</strong><li><a href="/">Home</a><li><a class="about" href="/about/">About Me</a><li><a class="about" href="/projects/">My Projects</a><li><a class="about" href="/feed.xml">XML Feed</a></ul></div></div><div class="col-sm-8 col-offset-1 main-layout"><header class="post-header"><h1 class="post-title">Machine Learning : Gradient Descent</h1></header><span class="time">16 Sep 2018</span> <span class="categories"> &raquo; <a href="/category/python">python</a>, <a href="/category/machine learning">machine learning</a> </span><div class="content"><div class="post"><p>This is one of the most questioned topics in Data Science interviews and one of the simplest methodologies to understand when starting to learn Machine Learning. Let’s finally understand what Gradient Descent is really all about!<p>According to Aurélien Géron’s book “Hands on Machine Learning with Scikit-Learn &amp; TensorFlow” (great book, for who is starting):<blockquote><p>Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea (…) is to tweak parameters iteratively in order to minimize a cost function.</blockquote><p>Before we go further into explaining in more detail Gradient Descent, it is important to understand what is a <strong>Cost Function</strong>.<h2 id="what-is-a-cost-function">What is a Cost Function?</h2><p>Let’s start this terminology by taking the simplest example of a Machine Learning algorithm: Linear Regression. Linear Regression is used to estimate linear relationships between continuous or/and categorical data and a continuous output variable.<p>//[ \hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + … + \theta_n x_n]//<ul><li>y^ is the predicted value<li>n is the number of features<li>xi is the ith feature value<li>θj is the jth model parameter (including the bias term θ0 and the feature weights θ1, θ2, ⋯, θn).</ul><p>To make things simpler, we’ll assume a practical example. You can imagine that our predicted value y is sales while X is advertising spends. We want to create a model that can estimate how the advertising budget impacts sales.<p><img src="https://cdn-images-1.medium.com/max/800/0*WE6CHQ_5DcwxIXVX.png" alt="" /> Our linear model (red) generalising to all the data available (blue dots). Wikipedia<p>Now, what our model tried to best estimate the parameter θj that would generalise for future advertising budget data, allowing us to make good predictions on the sales return. Cost Functions are used to measure how wrong the model is in terms of its ability to estimate the relationship between our advertising budget X and our target variable, sales y. Usually, the cost function is described as the difference between the predicted value given by our model and the true value.<p><img src="https://cdn-images-1.medium.com/max/800/1*oXPGYqgTeIn0Ey3SWgkbsA.jpeg" alt="" /> Difference or distance between the predicted value (red) and the actual value (blue). <img src="https://towardsdatascience.com/simple-linear-regression-2421076a5892" alt="Source" /><blockquote><p>Therefore, the goal of a Machine Learning algorithm is to find the best parameters θj which will minimise the cost function.</blockquote><p><img src="https://cdn-images-1.medium.com/max/800/1*XigwiTBItRFWwcW5H6ug2A.png" alt="" /><img src="https://developers.google.com/machine-learning/crash-course/reducing-loss/an-iterative-approach#ml-block-diagram" alt="Source" /><h2 id="gradient-descent">Gradient Descent:</h2><p>Minimising the Cost Function Hopefully, you’re now more enlightened on what is a cost function and we can go on explaining why gradient descent is so important. As previously said, gradient descent is an efficient optimisation algorithm that attempts to find the global minima of a cost function. According to Aurélien, the gradient descent “measures the local gradient of the error function with regards to the parameter vector θj, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached the minimum.” For most regression problems, the resulting plot of “Cost Function vs Parameter θ” will always be convex therefore having only a single minimum where the slope is exactly zero and also where the cost function converges.<p><img src="https://cdn-images-1.medium.com/max/800/1*pWeXDYsXgL0GuX8pbIl5QA.png" alt="" /><p>Calculating the cost function parameters for all θj over the entire data set would be an inefficient way of finding the convergence point. <strong>This is where gradient descent does its magic! But how does it work?</strong><p>You start by randomly fill the parameter θ with random values (also known as random initialisation). At this point, the gradient descent algorithm calculates the gradient of the loss curve at the starting point, which is the derivative (slope) of the curve. In the end, it gives you the direction of your next step.<p><img src="https://cdn-images-1.medium.com/max/800/1*WM2pDoJI8XMiCPmm1J4r8A.png" alt="" /><img src="https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent" alt="Source" /><p>You repeat this process incrementally, each step at a time, trying to reduce the cost function until your algorithm converges to a minimum as shown in this gif below.<p><img src="https://cdn-images-1.medium.com/max/800/0*25f0lTBwj-PFvm66.gif" alt="" />Cost Function vs Value of Weight (parameter θ). <img src="https://developers.google.com/machine-learning/crash-course/fitter/graph" alt="Source" /><p>At this point (local minimum) the model has optimised the parameters such that it minimise to a minimum the cost function.<p>Importance of the Learning Rate<p>We’ve seen so far that the gradient vector has both a direction and a magnitude (red arrow). The gradient descent algorithm multiplies the gradient by a scalar known as learning rate (or step size). Hence, the learning rate is the hyperparameter that the algorithm uses to converge either by taking small steps (much more computational time) or larger steps. See the following gif examples to understand the impact of selecting different learning rates.<p>Learning rate = 0.10<p><img src="https://cdn-images-1.medium.com/max/1000/0*En4lt8S2kEwtSkjV.gif" alt="" /><p>Learning rate = 1.0<p><img src="https://cdn-images-1.medium.com/max/800/0*Qlv8qC8CGS8alBjf.gif" alt="" /><p>Learning rate = 1.6 <img src="https://cdn-images-1.medium.com/max/1000/0*HF7mFEjKtGfn6HxW.gif" alt="" /><p>The examples show great learning rates that allowed us to achieve the cost function convergence point slower or faster. Nevertheless, just be careful when choosing higher learning rates since this might take the algorithm to diverge, with larger and larger values, failing to find a good solution. Take a look at the image below for an example.<p><img src="https://cdn-images-1.medium.com/max/800/0*QwE8M4MupSdqA3M4.png" alt="" /><p>Moreover, you should also have in mind that not all cost functions look like nice regular bowls. That usually happens for Linear Regression, and it is great because regardless of your initial assumption for parameter θj it will always converge. With other Machine Learning algorithms such as Neural Networks you might find cost functions with irregularities making it hard to converge.<p>Take into consideration the image below. If our random initialisation for our parameter θj starts on the left our algorithm will converge to a minimum which is not the best. However, if it starts on the right it will take longer to do the plateau and eventually might reach the right minimum if you do not stop early<p><img src="https://cdn-images-1.medium.com/max/800/0*hsnJ0zBOdq6G_o8J" alt="" /><blockquote><p>An important fact you should have in mind is that all features should have similar scales in order to ensure the algorithm will not take more than necessary to converge.</blockquote><h2 id="the-types-of-gradient-descent">The types of Gradient Descent</h2><p>There are three popular gradient descent types that mainly differ for the amount of data they handle.<h3 id="batch-gradient-descent">Batch Gradient Descent</h3><p>In gradient descent, the batch is the total number of examples you use to calculate the gradient in a single iteration. This algorithm does its calculations over the full training dataset, at each step of the gradient descent. Hence, it uses the whole dataset at every step, making it very very very slow for large datasets.<p>Pros: Computational efficient, since it produces a stable error gradient and a stable convergence.<p>Cons: Requires that the training dataset is in memory and available to the algorithm. It is slow since it uses the whole training set to compute the gradient at every learning step.<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3><p>This alternative to Batch Gradient Descent, it is on the other extreme of the idea, using a single example (batch of 1) per each learning step. Of course, due to this methodology ,the algorithm is much faster since it has few data to manipulate at every iteration, however it can return very noisy gradients which can cause the error rate to jump around. Therefore, due to this jumping around when the algorithm stops instead of finding the optimal fit it ends up obtaining only good results.<p>Pros: Constant update allows us to have a detailed rate of improvement. Allows usage of huge datasets since only one instance is needed to be allocated to the memory at each step.<p>When the cost function is very irregular such as in the last image, due to the jumping around this algorithm behaves better.<p>Cons: Due to its stochastic (i.e. random) nature, the algorithm is much less regular than the previous one. Instead of slowly decreasing the cost function until it reaches the desired value, the Stochastic bounces up and down only decreasing on average. Jumping around is good to escape local minimum but bad because hardly the algorithm will ever settle at the global minimum.<h3 id="mini-batch-gradient-descent">Mini Batch Gradient Descent</h3><p>This is the go-to method! Contrary to the two last gradient descent algorithms that we saw, instead of using the full dataset or a single instance, the Mini Batch, as the name indicates ,computes the gradients on small random sets of instances called mini batches.<p>This algorithm is able to reduce the noise from the Stochastic and still be more efficient than full-batch. Basically, it splits the training dataset into small batches ranging from 10 to 1,000 examples, chosen at random.<p>This algorithm is the best option when you are training a neural network and it is the most common within the Deep Learning field.<p>Resources :<ul><li>Hands-on Machine Learning with Scikit-Learn &amp; TensorFlow by Aurélien Géron<li>Google’s Machine Learning Crash Course<li><img src="https://medium.com/diogo-menezes-borges/what-is-gradient-descent-235a6c8d26b0" alt="What is Gradient Descent" /></ul></div><div class="share-page"> <span style="float: left;">Share this on &rarr;&nbsp;&nbsp;</span> <a href="https://twitter.com/share" class="twitter-share-button" data-via="github.io">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script><div class="fb-share-button" data-href="https://mragungsetiaji.github.io/python/machine%20learning/2018/09/16/gradient-descent.html" data-layout="button_count" style="position: relative; top: -8px; left: 3px;"></div></div><div id="fb-root"></div><script>(function(d, s, id) { var js, fjs = d.getElementsByTagName(s)[0]; if (d.getElementById(id)) return; js = d.createElement(s); js.id = id; js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.6&appId=1749788565247320"; fjs.parentNode.insertBefore(js, fjs); }(document, 'script', 'facebook-jssdk'));</script></div><div class="panel-body"><h4>Related Posts</h4><ul><li class="relatedPost"> <a href="https://mragungsetiaji.github.io/python/machine%20learning/2018/09/21/embedding.html">Machine Learning : Embedding</a> (Categories: <a href="/category/python">python</a>, <a href="/category/machine learning">machine learning</a>)<li class="relatedPost"> <a href="https://mragungsetiaji.github.io/python/machine%20learning/2018/09/21/machine-learning-accuracy-recall-dan-precision.html">Machine Learning : Acccuracy, Recall & Precision</a> (Categories: <a href="/category/python">python</a>, <a href="/category/machine learning">machine learning</a>)<li class="relatedPost"> <a href="https://mragungsetiaji.github.io/python/machine%20learning/2018/09/11/deeplearning-transferlearning.html">Deep Learning : Transfer Learning</a> (Categories: <a href="/category/python">python</a>, <a href="/category/machine learning">machine learning</a>)<li class="relatedPost"> <a href="https://mragungsetiaji.github.io/python/machine%20learning/2018/09/02/deeplearning-loss-function-metric.html">Deep Learning : Loss Function, Metric</a> (Categories: <a href="/category/python">python</a>, <a href="/category/machine learning">machine learning</a>)<li class="relatedPost"> <a href="https://mragungsetiaji.github.io/python/machine%20learning/2018/09/01/deeplearning-epoch-batch-iteration.html">Deep Learning : Epoch, Batch dan Iteration</a> (Categories: <a href="/category/python">python</a>, <a href="/category/machine learning">machine learning</a>)<li class="relatedPost"> <a href="https://mragungsetiaji.github.io/python/machine%20learning/2018/09/01/deeplearning-activation-function.html">Deep Learning : Activation Function</a> (Categories: <a href="/category/python">python</a>, <a href="/category/machine learning">machine learning</a>)</ul></div><div class="PageNavigation"> <a class="prev" href="/python/machine%20learning/2018/09/11/deeplearning-transferlearning.html">&laquo; Deep Learning : Transfer Learning</a> <a class="next" href="/python/machine%20learning/2018/09/21/machine-learning-accuracy-recall-dan-precision.html">Machine Learning : Acccuracy, Recall & Precision &raquo;</a></div><div class="disqus-comments"><div id="disqus_thread"></div><script type="text/javascript"> /* <![CDATA[ */ var disqus_shortname = "mragungsetiaji"; var disqus_identifier = "https://mragungsetiaji.github.io_Machine Learning : Gradient Descent"; var disqus_title = "Machine Learning : Gradient Descent"; /* * * DON'T EDIT BELOW THIS LINE * * */ (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); /* ]]> */ </script></div><footer> &copy; Agung Setiaji - <a href="https://github.com/mragungsetiaji">https://github.com/mragungsetiaji</a> - Powered by Jekyll.<div class="btn-github" style="float:right;"> <iframe src="https://ghbtns.com/github-btn.html?user=mragungsetiaji&repo=mragungsetiaji.github.io&type=star&count=true" frameborder="0" scrolling="0" width="85" height="20px"></iframe> <iframe src="https://ghbtns.com/github-btn.html?user=mragungsetiaji&repo=mragungsetiaji.github.io&type=fork&count=true" frameborder="0" scrolling="0" width="85" height="20px"></iframe></div></footer></div></div><script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script> <script src="//code.jquery.com/jquery-migrate-1.2.1.min.js"></script> <script src="/static/js/bootstrap.min.js"></script> <script src="/static/js/super-search.js"></script> <script src="/static/js/thickbox-compressed.js"></script> <script src="/static/js/projects.js"></script>
